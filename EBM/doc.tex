\documentclass{article}

\usepackage{authblk}
% \usepackage{hyperref}
\usepackage{natbib}

\input{../general.tex}
% \usepackage[UTF8]{ctex} % Chinese support


\title{\textbf{\Large Miscellanea}}
\author{Jiachun Jin}
\affil{\textit{\small School of Information Science and Technology}}
\affil{\textit{\small ShanghaiTech University}}
\date{}

\begin{document}
\maketitle
This is a my daily research note. The goal is to make things as clear as possible.

\tableofcontents

\section{Energy-Based Models}
[Here I should give more high level discussion about EBMs] \citep{lecun2006tutorial, song2021train}.

The density given by an EBM is:
\begin{equation}
    p_\vtheta(\rvx) = \frac{\exp\left( -E_\vtheta(\rvx) \right)}{Z(\vtheta)},
\end{equation}
where $ E_\vtheta(\rvx) $ is called the energy, and $ Z_{\vtheta} = \int \exp\left( -E_\vtheta(\rvx) \right)$ denotes the normalizing constant, which is intractable.
\subsection{Training with MLE}
Maximum likelihood estimation (MLE) is the \textit{de facto} standard for learning probabilistic models from i.i.d. data. The gradient of the log likelihood with respect to the model parameter $\vtheta$ is given by:
\begin{equation}\label{eq.MLE}
    \begin{aligned}
        \nabla_{\vtheta}\log p_\vtheta(\rvx)
        &= -\nabla_\vtheta E_\vtheta(\rvx) - \nabla_\vtheta \log Z_\vtheta \\
        &= -\nabla_\vtheta E_\vtheta(\rvx) - \nabla_\vtheta \log \int \exp\{ -E_\vtheta(\rvx) \}\d\rvx \\
        &= -\nabla_\vtheta E_\vtheta(\rvx) - \frac{1}{Z_\vtheta} \int \nabla_\vtheta \exp\{ -E_\vtheta(\rvx) \}\d\rvx \\
        &= -\nabla_\vtheta E_\vtheta(\rvx) + \int p_\vtheta(\rvx)\nabla_\vtheta E_\vtheta(\rvx)\d\rvx \\
        &= \underbrace{-\nabla_\vtheta E_\vtheta(\rvx)}_{\text{positive phase}} + \underbrace{\E_{\tilde{\rvx} \sim p_\vtheta(\tilde\rvx)}\left[ \nabla_\vtheta E_\vtheta(\tilde\rvx) \right]}_{\text{negative phase}},
    \end{aligned}
\end{equation}
in the third line, we switch the order of gradient and integral, the details about the validity is discussed in \secref{Asec.switchintgrad}. The positive phase tries to decrease the energy of real data samples, and the negative phase tries to increase the energy of sample generated by the current model (this can be considered as reducing the model's incorrect beliefs about the world, which is analogous to what human beings do when they are dreaming) \citep[Section~18.2]{deeplearningbook}. The "wake-sleep" fashion can be used in approximate inference settings \citep[Section~19.5]{deeplearningbook}.

The difficulty is we need to sample from the model, which is unnormalized, and MCMC algorithms like the Langevin Dynamic is utilized. Running MCMC until convergence is computationally expensive. Contrastive Divergence (CD) \citep{CDhinton} and persistent CD \citep{PCD} are some alternative methods to approximate the gradient by some short run MCMC iterations.

\subsubsection{Contrastive Divergence}\label{sec.CD}
The main difficulty in ML training is in the negative phase, where samples from the model are needed. \citeauthor{CDhinton} proposed the Contrastive Divergence method to use samples from $p^{(1)}_{\vtheta}(\rvx) = \Pi_\vtheta^{(1)}\pdata(\rvx)$, which stands for the distribution by running $1$ step MCMC iteration over the samples from $\pdata(\rvx)$. Thus the gradient in \eqref{eq.MLE} over the training data becomes to:
\begin{equation}\label{eq.CD}
    \begin{aligned}
        \E_{\pdata(\rvx)}\left[  \nabla_{\vtheta}\log p_\vtheta(\rvx) \right]
        &= \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{\tilde{\rvx} \sim p_\vtheta(\tilde\rvx)}\left[ \nabla_\vtheta E_\vtheta(\tilde\rvx) \right] \\
        &\approx \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{\tilde{\rvx} \sim p^{(1)}_{\vtheta}(\tilde\rvx)}\left[ \nabla_\vtheta E_\vtheta(\tilde\rvx) \right].
    \end{aligned}
\end{equation}
The mathematical motivation for such substitution is that the CD method is actually approximately minimizing the following objective:
\begin{equation}
    \KL\left[ \pdata(\rvx)\parallel p_\vtheta(\rvx) \right] - \KL\left[ p^{(1)}_{\vtheta}(\rvx) \parallel p_\vtheta(\rvx)\right],
\end{equation}
and the gradient of $\vtheta$ w.r.t. it is:
\begin{equation}\label{eq.gradCD}
    \begin{aligned}
        &\quad \nabla_\vtheta\left[ \KL\left[ \pdata(\rvx)\parallel p_\vtheta(\rvx) \right] - \KL\left[ p^{(1)}_{\vtheta}(\rvx') \parallel p_\vtheta(\rvx')\right] \right] \\
        &= -\E_{\pdata(\rvx)}\left[ \nabla_\vtheta\log p_\vtheta(\rvx) \right] - 
        \underbrace{\nabla_\vtheta p_\vtheta^{(1)}(\rvx')\frac{\partial}{\partial p_\vtheta^{(1)}(\rvx')}\KL\left[ p^{(1)}_{\vtheta}(\rvx') \parallel p_\vtheta(\rvx')\right]}_{\text{\textcircled{1}}} - 
        \underbrace{\nabla_\vtheta p_\vtheta(\rvx')\frac{\partial}{\partial p_\vtheta(\rvx')}\KL\left[ p^{(1)}_{\vtheta}(\rvx') \parallel p_\vtheta(\rvx')\right]}_{\text{\textcircled{2}}}, \\
    \end{aligned}
\end{equation}
let's examine \textcircled{2}:
\begin{equation}
    \begin{aligned}
        \nabla_\vtheta p_\vtheta(\rvx')\frac{\partial}{\partial p_\vtheta(\rvx')}\KL\left[ p^{(1)}_{\vtheta}(\rvx') \parallel p_\vtheta(\rvx')\right]
        &= \nabla_\vtheta p_\vtheta(\rvx')\int p^{(1)}_{\vtheta}(\rvx')\frac{\partial}{\partial p_\vtheta(\rvx')}\left( -\log p_\vtheta(\rvx') \right)\d\rvx' \\
        &= -\int p^{(1)}_{\vtheta}(\rvx')\frac{\nabla_\vtheta p_\vtheta(\rvx')}{p_\vtheta(\rvx')}\d\rvx' \\
        &= -\int p^{(1)}_{\vtheta}(\rvx') \nabla_\vtheta\log p_\vtheta(\rvx')\d\rvx' \\
        &= -\E_{p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta\log p_\vtheta(\rvx') \right].
    \end{aligned}
\end{equation}
And we can see if we simply drop \textcircled{1}, then \eqref{eq.gradCD} becomes to:
\begin{equation}
        -\E_{\pdata(\rvx)}\left[ \nabla_\vtheta\log p_\vtheta(\rvx) \right] + \E_{p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta\log p_\vtheta(\rvx') \right] = \E_{\pdata(\rvx)}\left[ \nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{p^{(1)}_{\vtheta}(\rvx')}\left[ -\nabla_\vtheta E_\vtheta(\rvx') \right],
\end{equation}
which is just the negative of \eqref{eq.CD}. Although the discarded \textcircled{1} makes CD a biased algorithm, the bias is always small. Recently improved CD \citep{improvedCD} takes this term into consideration and makes the training procedure more stable.


\subsection{Training with Score Matching}
The key observation is the score of the EBM $\nabla_{\rvx}\log p_\vtheta(\rvx) = -\nabla_{\rvx}E_\vtheta(\rvx)$ is independent of the partition function $Z_\vtheta$. Thus when we try to minimize the Fisher divergence between our data and the EBM, we can avoid dealing with the intractable parition $Z_\vtheta$:
\begin{equation}
    \Fisher(\pdata(\rvx) \parallel p_\vtheta(\rvx)) = \E_{\pdata(\rvx)}\left[ \frac{1}{2}\left\|\nabla_{\mathbf{x}} \log \pdata(\mathbf{x})-\nabla_{\mathbf{x}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\right\|^2 \right].
\end{equation}

\subsubsection{Basic Score Matching}\label{sec.SM}
The problem is that $\nabla_\rvx \log \pdata(\rvx)$ is unknown. However, with integration by parts, the second derivatives of $E_\vtheta(\rvx)$ can be used to replace the unknown $\nabla_\rvx \log \pdata(\rvx)$ \citep{hyvarinen2005estimation}.
\begin{equation}
    \begin{aligned}
        \mathcal{J(\vtheta)} &= \E_{\pdata(\rvx)}\left[ \frac{1}{2}\left\|\nabla_{\mathbf{x}} \log \pdata(\mathbf{x})-\nabla_{\mathbf{x}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\right\|^2 \right] \\
        &= \int \pdata(\rvx)\left[ \frac{1}{2}\| \nabla_\rvx \log \pdata(\rvx) \|^2 + \frac{1}{2} \| \nabla_\rvx\log p_\vtheta(\rvx) \|^2 - \nabla_\rvx \log \pdata(\rvx)^\top \nabla_\rvx\log p_\vtheta(\rvx) \right]\d \rvx \\
        &= \int \pdata(\rvx)\left[ \frac{1}{2} \| \nabla_\rvx\log p_\vtheta(\rvx) \|^2 \right] \d\rvx - \underbrace{\int \pdata(\rvx) \left[ \nabla_\rvx \log \pdata(\rvx)^\top \nabla_\rvx\log p_\vtheta(\rvx) \right] \d\rvx}_{\text{\textcircled{1}}}  + \text{const},
    \end{aligned}
\end{equation}
the computational difficulty exists in term \textcircled{1}, and we can conquer that with integration by parts, in the following we use $d$ to denote the data dimensionality:
\begin{equation}
    \begin{aligned}
        \text{\textcircled{1}}
        &= \sum_{i=1}^{d}\int \pdata(\rvx)\frac{\partial}{\partial \rvx_i}\log \pdata(\rvx)\frac{\partial}{\partial \rvx_i}\log p_\vtheta(\rvx)\d \rvx \\
        &= \sum_{i=1}^d \int \frac{\partial}{\partial \rvx_i}\pdata(\rvx) \frac{\partial}{\partial \rvx_i}\log p_\vtheta(\rvx)\d \rvx,
    \end{aligned}
\end{equation}
and without loss of generality, we can examine the first term in the summation:
\begin{equation}\label{eq.IBP}
    \begin{aligned}
        &\quad\int \frac{\partial}{\partial \rvx_1}\pdata(\rvx) \frac{\partial}{\partial \rvx_1}\log p_\vtheta(\rvx)\d \rvx \\
        &= \int \left( \int  \frac{\partial}{\partial \rvx_1}\pdata(\rvx) \frac{\partial}{\partial \rvx_1}\log p_\vtheta(\rvx)\d \rvx_1 \right) \d \rvx_2\cdots\rvx_d \\
        &= \int \left[ f(\rvx_{2:d}) - \int \pdata(\rvx)\frac{\partial}{\partial \rvx_1}\frac{\partial}{\partial \rvx_1}\log p_\vtheta(\rvx)\d\rvx_1\right]\d \rvx_2\cdots\rvx_d \\
        &= -\int \pdata(\rvx) \frac{\partial^2}{\partial \rvx_1^2}\log p_\vtheta(\rvx)\d\rvx,
    \end{aligned}
\end{equation}
where 
$$
f(\rvx_{2:d}) = \lim_{a\to\infty, b\to-\infty}\left( \pdata(a,\rvx_{2:d})\frac{\partial}{\partial \rvx_1}\log p_\vtheta(a, \rvx_{2:d}) - \pdata(b,\rvx_{2:d})\frac{\partial}{\partial \rvx_1}\log p_\vtheta(b,\rvx_{2:d})\right),
$$
and this term is assumed to be zero since the regularity condition of our model is: (1) $\pdata(\rvx)\frac{\partial}{\partial \rvx}\log p_\vtheta(\rvx)$ goes to zero for any $\vtheta$ when $\|\rvx\| \to \infty$, (2) $\pdata(\rvx)$ is differentiable, (3) $\E_{\pdata(\rvx)}\left[ \| \nabla_\rvx \log p_\vtheta(\rvx) \|^2 \right]$ and $\E_{\pdata(\rvx)}\left[ \| \nabla_\rvx \log \pdata(\rvx) \|^2 \right]$ are finite for every $\vtheta$. The third line of \eqref{eq.IBP} comes from integration by parts, which can move the $\frac{\partial}{\partial \rvx_1}$ from $\pdata(\rvx)$ to $\frac{\partial}{\partial \rvx_1}\log p_\vtheta(\rvx)$.

With the above derivation, we have:
\begin{equation}\label{eq.BSM}
    \begin{aligned}
        \mathcal{J}(\vtheta)
        &= \int \pdata(\rvx)\left[ \frac{1}{2} \| \nabla_\rvx\log p_\vtheta(\rvx) \|^2 \right] \d\rvx + \sum_{i=1}^d\int \pdata(\rvx)\frac{\partial^2}{\partial \rvx_1^2}\log p_\vtheta(\rvx)\d\rvx + \text{const} \\
        &= \int \pdata(\rvx)\sum_{i=1}^d \left( \frac{1}{2}\left( \frac{\partial}{\partial \rvx_i}\log p_\vtheta(\rvx) \right)^2 + \frac{\partial^2}{\partial \rvx_i^2}\log p_\vtheta(\rvx) \right)\d\rvx + \text{const} \\
        &= \E_{\pdata(\rvx)}\left[ \sum_{i=1}^d\frac{1}{2}\left( \frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx) \right)^2 - \frac{\partial^2}{\partial \rvx_i^2}E_\vtheta(\rvx)\right] + \text{const}.
    \end{aligned}
\end{equation}
In this way, we can learn the EBM when we can compute the score and Hessian of the energy function $E_\vtheta(\rvx)$.

\subsubsection{Denoising Score Matching}
There two main shortcomings of basic SM. First, it is only applicable to continuous and unbounded data, which cannot be used to digital data. Second is the computational cost of Hessian is $\mathcal{O}(d)$, and can not be applied to high dimensional data. One way to alleviate the problem is to add some smooth noise $\bm\epsilon$ to the data, and the resulting noisy distribution is $q(\tilde \rvx) = \int q(\tilde \rvx | \rvx)\pdata(\rvx) \d\rvx$. The interesting is:
\begin{equation}\label{eq.DSMFisher}
    \begin{aligned}
        \Fisher(q(\tilde{\rvx})\parallel p_\vtheta(\tilde{\rvx}))
        &= \E_{q(\tilde{\rvx})}\left[ \frac{1}{2} \| \nabla_{\tilde{\rvx}}\log q(\tilde{\rvx}) - \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2 \right] \\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] 
        - \E_{q(\tilde{\rvx})}\left[ \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top \nabla_{\tilde{\rvx}}\log q(\tilde{\rvx})\right] + \text{const}\\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] - 
        \int \frac{q(\tilde{\rvx})}{q(\tilde{\rvx})}\left( \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top \nabla_{\tilde{\rvx}}q(\tilde{\rvx}) \right)\d\tilde{\rvx} + \text{const}\\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] - 
        \int \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top \int \nabla_{\tilde{\rvx}}q(\tilde{\rvx}|\rvx)\pdata(\rvx)\d\rvx\d\tilde{\rvx} + \text{const}\\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] - 
        \int \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top \int q(\tilde{\rvx}|\rvx)\nabla_{\tilde{\rvx}}\log q(\tilde{\rvx}|\rvx)\pdata(\rvx)\d\rvx\d\tilde{\rvx} + \text{const}\\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] - 
        \int \int q(\tilde{\rvx}|\rvx)\pdata(\rvx)\nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top\nabla_{\tilde{\rvx}}\log q(\tilde{\rvx}|\rvx)\d\rvx\d\tilde{\rvx} + \text{const}\\
        &= \E_{q(\tilde\rvx)}\left[ \frac{1}{2}\| \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx}) \|^2\right] - 
        \E_{q(\tilde{\rvx}, \rvx)} \left[ \nabla_{\tilde{\rvx}}\log p_\vtheta(\tilde{\rvx})^\top\nabla_{\tilde{\rvx}}\log q(\tilde{\rvx}|\rvx) \right] + \text{const}\\
        &= \mathbb{E}_{q(\tilde{\rvx}, \rvx)}\left[\frac{1}{2}\left\|\nabla_{\tilde{{\rvx}}} \log q(\tilde{\mathbf{x}} | \mathbf{x})-\nabla_{\tilde{\rvx}} \log p_{\boldsymbol{\theta}}(\tilde{\mathbf{x}})\right\|_2^2\right] + \text{const},
    \end{aligned}
\end{equation}
we can see in the last line of \eqref{eq.DSMFisher}, both $\pdata(\rvx)$ and the second derivative of $p_\vtheta
(\rvx)$ are avoided. The underlying intuition is that following the gradient of $\log p_\vtheta(\tilde{\rvx})$  at some corrupted point $\tilde{\rvx}$ should ideally move us towards the clean sample $\rvx$ \citep{vincent2011connection}. One thing should be kept in mind is the learned score corresponds to the noisy data distribution $q(\tilde{\rvx})$ rather than the original noise-free $\pdata(\rvx)$, which makes DSM not a consistent estimator of $\pdata(\rvx)$. But we can attenuate the inconsistency to choose small noise level to make $q(\tilde{\rvx}) \approx \pdata(\rvx)$.

\subsubsection{Sliced Score Matching}
TODO

\subsection{Contrastive Divergence and Score Matching}
Surprisingly, there exist some close connections \citep{hyvarinen2007connections} between Contrastive Divergence (\secref{sec.CD}) and Score Matching (\secref{sec.SM}). To see this, suppose we only run one step Langevin MCMC for CD, that is:
\begin{equation}
    \rvx'(\textcolor{red}{\vtheta_s}) = \rvx - \frac{\epsilon^2}{2}\nabla_\rvx E_{\textcolor{red}{\vtheta_s}}(\rvx) + \epsilon \rvz, \quad \rvx \sim \pdata(\rvx), \rvz \sim \mathcal{N}(\rvz; 0, I),
\end{equation}
here $\textcolor{red}{\vtheta_s}$ denotes the current state of $\vtheta$, and each $\rvx'$ depends on $\textcolor{red}{\vtheta_s}$. The Taylor series expansion of $E_\vtheta(\rvx')$ at $\rvx$ is given by:
\begin{equation}\label{eq.Taylor}
    E_\vtheta(\rvx') = E_\vtheta(\rvx) + \nabla_\rvx^\top E_\vtheta(\rvx) (\rvx'- \rvx) + \frac{1}{2}(\rvx'- \rvx)^\top \nabla_\rvx^2 E_\vtheta(\rvx) (\rvx'- \rvx) + o(\epsilon^2),
\end{equation}
the corresponding CD gradient is given in the second line of \eqref{eq.CD}:
\begin{equation}
    \begin{aligned}
        &\quad \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta E_\vtheta(\rvx') \right] \\
        &= \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{\rvz \sim \mathcal{N}(\rvz; 0, I)}\left[ \nabla_\vtheta E_\vtheta\left(\rvx - \frac{\epsilon^2}{2}\nabla_\rvx E_{\textcolor{red}{\vtheta_s}}(\rvx) + \epsilon \rvz \right) \right].
    \end{aligned}
\end{equation}
From \eqref{eq.Taylor}, we can further have:
\begin{equation}
    \begin{aligned}
        &\quad -E_\vtheta(\rvx) + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ E_\vtheta(\rvx') \right] \\
        &= \E_{\rvz \sim \mathcal{N}(0, I)}\left[ -E_\vtheta(\rvx) + E_\vtheta(\rvx') \right]  \\
        &= \E_{\rvz \sim \mathcal{N}(0, I)}\left[ \nabla_\rvx^\top E_\vtheta(\rvx) (\rvx'- \rvx) + \frac{1}{2}(\rvx'- \rvx)^\top \nabla_\rvx^2 E_\vtheta(\rvx) (\rvx'- \rvx) + o(\epsilon^2) \right] \\
        &= \E_{\rvz \sim \mathcal{N}(0, I)}\left[ \nabla_\rvx^\top E_\vtheta(\rvx)\underbrace{\left( - \frac{\epsilon^2}{2}\nabla_\rvx E_{\textcolor{red}{\vtheta_s}}(\rvx) + \epsilon \rvz \right)}_{\text{A}} + \frac{1}{2}\text{A}^\top \nabla_\rvx^2 E_\vtheta(\rvx)\text{A} + o(\epsilon^2)\right] \\
        &= -\frac{\epsilon^2}{2}\nabla_\rvx^\top E_{\vtheta}(\rvx) \nabla_\rvx E_{{\textcolor{red}{\vtheta_s}}}(\rvx) + \frac{\epsilon^2}{2}\trace(\nabla_\rvx^2E_\vtheta(\rvx)) + o(\epsilon^2) \\
        &\approx \frac{\epsilon^2}{2}\left( -\nabla_\rvx^\top E_\vtheta(\rvx) \nabla_\rvx E_{\textcolor{red}{\vtheta_s}}(\rvx) + \trace(\nabla_\rvx^2E_\vtheta(\rvx))\right) \\
        &= -\frac{\epsilon^2}{2}\left(\nabla_\rvx^\top\log p_\vtheta(\rvx)\nabla_\rvx\log p_\vtheta(\rvx) - \nabla_\rvx^2\log p_\vtheta(\rvx)\right),
    \end{aligned}
\end{equation}
we can further denote:
\begin{equation}
    \begin{aligned}
        \mathcal{J}_{\text{CD}}(\rvx, \vtheta, \textcolor{red}{\vtheta_s}) 
        &= \frac{\epsilon^2}{2}\left( -\nabla_\rvx^\top E_\vtheta(\rvx) \nabla_\rvx E_{\textcolor{red}{\vtheta_s}}(\rvx) + \trace(\nabla_\rvx^2E_\vtheta(\rvx))\right) \\
        &= \frac{\epsilon^2}{2}\left(\sum_{i=1}^d -\frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx)\frac{\partial}{\partial \rvx_i}E_{\textcolor{red}{\vtheta_s}}(\rvx) + \frac{\partial^2}{\partial \rvx_i^2}E_\vtheta(\rvx)\right),
    \end{aligned}
\end{equation}
notice we always take $\textcolor{red}{\vtheta_s}$ as a constant, then:
\begin{equation}
    \frac{\partial}{\partial \vtheta_k} \E_{\pdata(\rvx)}\left[ \mathcal{J}_{\text{CD}}(\rvx, \vtheta, \textcolor{red}{\vtheta_s}) \right] = \frac{\epsilon^2}{2}\E_{\pdata(\rvx)}\left[\sum_{i=1}^d -\frac{\partial}{\partial \vtheta_k} \frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx)\frac{\partial}{\partial \rvx_i}E_{\textcolor{red}{\vtheta_s}}(\rvx) + \frac{\partial}{\partial \vtheta_k}\frac{\partial^2}{\partial \rvx_i^2}E_\vtheta(\rvx)\right],
\end{equation}
% then the gradient used in CD to update $\vtheta$ becomes to:
% \begin{equation}
%     \begin{aligned}
%         \E_{\pdata(\rvx)}\left[ \nabla_\vtheta\log p_\vtheta(\rvx) \right] 
%         &\approx \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta E_\vtheta(\rvx') \right] \\
%         &= \nabla_\vtheta\left( \E_{\pdata(\rvx)}\left[ -E_\vtheta(\rvx) \right] + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ E_\vtheta(\rvx') \right] \right) \\
%         &\approx \nabla_\vtheta \E_{\pdata(\rvx)}\left[ \frac{\epsilon^2}{2}  \left( -\nabla_\rvx^\top E_\vtheta(\rvx) \nabla_\rvx E_\vtheta(\rvx) + \trace(\nabla_\rvx^2E_\vtheta(\rvx))\right) \right] \\
%         &= \frac{\epsilon^2}{2}\E_{\pdata(\rvx)}\left[\left( -\nabla_\vtheta\nabla_\rvx^\top E_\vtheta(\rvx)  \nabla_\rvx E_\vtheta(\rvx) + \nabla_\vtheta \trace(\nabla_\rvx^2E_\vtheta(\rvx)) \right) \right],
%     \end{aligned}
% \end{equation}
% \textcolor{red}{something more complex, refer to Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables eq.8}\\
and if we examine the gradient w.r.t. $\vtheta$ of score matching (\eqref{eq.BSM}), we have:
\begin{equation}
    \begin{aligned}
        &\quad \frac{\partial}{\partial \vtheta_k} \mathcal{J}(\vtheta) = \frac{\partial}{\partial \vtheta_k} \Fisher(\pdata(\rvx) \parallel p_\vtheta(\rvx)) \\
        &= \frac{\partial}{\partial \vtheta_k} \left( \E_{\pdata(\rvx)}\left[ \sum_{i=1}^d\frac{1}{2}\left( \frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx) \right)^2 - \frac{\partial^2}{\partial \rvx_i^2}E_\vtheta(\rvx)\right] + \text{const} \right) \\
        &= \E_{\pdata(\rvx)}\left[ \sum_{i=1}^d \frac{\partial}{\partial \vtheta_k}\frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx)\frac{\partial}{\partial \rvx_i}E_\vtheta(\rvx) - \frac{\partial}{\partial \vtheta_k} \frac{\partial^2}{\partial \rvx_i^2}E_\vtheta(\rvx)\right].
    \end{aligned}
\end{equation}
From the above and \eqref{eq.gradCD}, we can conclude that:
\begin{equation}\label{eq.connectionCDSM}
    \begin{aligned}
        \E_{\pdata(\rvx)}\left[ \nabla_\vtheta \log p_\vtheta(\rvx) \right]
        &\approx \E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta E_\vtheta(\rvx') \right] \\
        &\approx -\nabla_\vtheta\left[ \KL\left[ \pdata(\rvx)\parallel p_\vtheta(\rvx) \right] - \KL\left[ p^{(1)}_{\vtheta}(\rvx') \parallel p_\vtheta(\rvx')\right] \right] \\
        &\approx -\frac{\epsilon^2}{2}\nabla_\vtheta  \Fisher(\pdata(\rvx) \parallel p_\vtheta(\rvx)),
    \end{aligned}
\end{equation}
the approximation in the first line comes from using one step Langevin MCMC instead of infinite many, the one in the second line comes from dropping \textcircled{1} in \eqref{eq.gradCD}, and the one in the third line comes from dropping $o(\epsilon^2)$ from Taylor series expansion. 

What \eqref{eq.connectionCDSM} tells us is that every time we add $\E_{\pdata(\rvx)}\left[ -\nabla_\vtheta E_\vtheta(\rvx) \right] + \E_{{\rvx'} \sim p^{(1)}_{\vtheta}(\rvx')}\left[ \nabla_\vtheta E_\vtheta(\rvx') \right]$ multiplied by a stepsize to the current $\vtheta$ to do approximate MLE, we are implicitly and approximately doing gradient descent of the Fisher divergence between $\pdata(\rvx)$ and our model $p_\vtheta(\rvx)$.
\subsection{Training with Noise Contrastive Estimation}
(This closely related to density ratio estimation via binary classification.)

In NCE we introduce a noise distribution $p_n(\rvx)$ which we can sample from and evaluate density, for example we can choose $p_n(\rvx) = \mathcal{N}(\rvx; 0, I)$. We also introduce a pseudo binary label $y$ which is $1$ if the point is from $\pdata(\rvx)$ and is $0$ if the point is from $p_n(\rvx)$. Then we can define a mixture distribution of samples from $\pdata$ and $p_n$:
$$
p_{n, \mathrm{data}}(\rvx) = p(y = 1)\pdata(\rvx) + p(y = 0)p_n(\rvx),
$$
then the posterior of $y = 0$ is given by:
\begin{equation}
    \begin{aligned}
        p_{n, \mathrm{data}}(y = 0 \mid \rvx)
        &= \frac{p_{n, \mathrm{data}}(\rvx \mid y = 0)p(y = 0)}{p_{n, \mathrm{data}}(\rvx)} \\
        &= \frac{p_{n}(\rvx)p(y = 0)}{p(y = 1)\pdata(\rvx) + p(y = 0)p_n(\rvx)} \\
        &= \frac{p_{n}(\rvx)}{\nu\pdata(\rvx) + p_n(\rvx)},
    \end{aligned}
\end{equation}
where $\nu = p(y = 1) / p(y = 0)$. Similarly we can define a mixture distribution of $\pdata$ and $p_\vtheta$ and the posterior of $y = 0$ is given by:
\begin{equation}
    \begin{aligned}
        p_{n, \vtheta}(y = 0 \mid \rvx)
        &= \frac{p_{n}(\rvx)}{\nu p_\vtheta(\rvx) + p_n(\rvx)},
    \end{aligned}
\end{equation}
In NCE, we indirectly fit $p_{\vtheta}(\rvx)$ to $p_{\mathrm{data}}(\rvx)$ through maximizing:
\begin{equation}
    \E_{p_{n, \textrm{data}}(\rvx, y)}\left[ \log p_{n, \vtheta}(y \mid \rvx) \right],
\end{equation}
where $E_\vtheta(\rvx)$ and $Z_\vtheta$ are taken independently, in other words, there is no model restriction that $Z_\vtheta = \int \exp\left\{-E_\vtheta(\rvx)\right\} \d \rvx$. When the classifier is powerful enough, the optimal $p_{n, \vtheta^*}(y \mid \rvx)$ will recover $p_{n, \mathrm{data}}(y \mid \rvx)$ and $p_{\vtheta^*}(\rvx)$ will recover $\pdata(\rvx)$. Also, NCE provides the normalizing constant of an Energy-Based Model as a by-product of its training procedure.
% In NCE, we indirectly Ô¨Åt $p_{\vtheta}(\rvx)$ to $\pdata(\rvx)$ 
\subsubsection{relation with GAN}
The optimal classifier between two distinct distributions $p_1(\rvx), p_2(\rvx)$ , is given by: 
$$D^*(\rvx) = \frac{p_1(\rvx)}{p_1(\rvx) + p_2(\rvx)} = \frac{1}{1 + \exp\left( -\log \frac{p_2(\rvx)}{p_1(\rvx)} \right)} = \sigma(-\log \frac{p_2(\rvx)}{p_1(\rvx)}).$$
Where $\sigma(\cdot)$ denotes the sigmoid function. In the above, we assume that the prior probability that $p(D=1) = p(D=2) = 0.5$.
This optimal classifier assumes that we use logistic regression to train the classifier. In the reverse direction, we have that logistic regression gives us a method to estimate the density which transform the density (ratio) estimation problem into a classification problem.
\textbf{what is aggregated posterior?}\\

\subsection{Adversarial Training of EBMs}

\subsection{Minimum Probability Flow Learning}


\subsection{Refer to the deep learning book of confront partition function}


\subsection{What is Pseudolikelihood?}

\section{A Line of Works Based on DRE}
\begin{itemize}
    \item When learning an EBM with NCE, use an adaptive noise distribution such as a normalizing flow
    \item 
\end{itemize}
\begin{align}
    V(\theta, \alpha) =\mathbb{E}_{p_{\text {data }}}\left[\log \frac{p_\theta(x)}{p_\theta(x)+q_\alpha(x)}\right] +\mathbb{E}_z\left[\log \frac{q_\alpha\left(g_\alpha(z)\right)}{p_\theta\left(g_\alpha(z)\right)+q_\alpha\left(g_\alpha(z)\right)}\right]
\end{align}


\section{Latent generative model}
\citep{xiao2022adaptive}
In a LVGM:
$$
\nabla_\theta \log p_\theta(\rvx, \rvz) = \E_{p_\theta(\rvz | \rvx)}\left[ \nabla_\theta \log p_\theta(\rvx, \rvz) \right],
$$
two common ways to do MLE over the above model:
\begin{itemize}
    \item Define a variational distribution $q_\phi(\rvz | \rvx)$ to do approximate MLE.
    \item Do MCMC (like Langevin Dynamic) to get samples from the posterior.
\end{itemize}




\section{Energy-Based Latent Variable Models (EBLVMs)}
Introducing latent variables to EBMs is a good idea.

\section{Application of EBMs}
What kind of benefits can we get when using an unnormalized probabilistic model? In density estimation the benefit is natural.
\section{Reminders}
\begin{itemize}
    \item score matching and fisher divergence minimization
    \item tweedie's formula
    \item contrastive divergence
    \item Yingzhen's talk, connection with wake sleep, RBMs
    \begin{itemize}
        \item why actually do we really need $\Ls_{CD} = D_{KL}\left[ \pdata \parallel p_\theta \right] - D_{KL}\left[ \Pi \pdata \parallel p_\theta \right] $ rather than directly using $ \rvx^{(1)} $ in $ \E_{p_\theta}\left[ \frac{\partial E_\theta(\rvx^{\prime})}{\partial \theta} \right] $ 
        \item It seems that we use $ \Ls_{CD} $ as a proxy of the maximum likelihood objective (negative likelihood).
    \end{itemize}
    \item improved contrastive divergence
\end{itemize}
\begin{itemize}
    \item Maximum entropy model
    \item flexibility
    \item Fenchel inequality
    \item MLE training of EBMs: "analysis by synthesis"
\end{itemize}

\bibliographystyle{abbrvnat} 
\bibliography{reference}

\appendix
\section{Switching integral and derivative}\label{Asec.switchintgrad}
TODO

\end{document}
